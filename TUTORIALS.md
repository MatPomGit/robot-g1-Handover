# Tutoriale Krok po Kroku (Step-by-Step Tutorials)

## ğŸ“– Wprowadzenie

Ten dokument zawiera szczegÃ³Å‚owe tutoriale prowadzÄ…ce przez najwaÅ¼niejsze funkcjonalnoÅ›ci systemu Robot G1 Handover. KaÅ¼dy tutorial jest zaprojektowany jako samodzielne Ä‡wiczenie z jasno okreÅ›lonymi celami edukacyjnymi.

---

## ğŸ¯ Tutorial 1: Pierwsze Uruchomienie Systemu

**Cel**: UruchomiÄ‡ podstawowy pipeline i zrozumieÄ‡ przepÅ‚yw danych.

**Wymagania**: Ubuntu 22.04, ROS 2 Humble

**Czas**: 30 minut

### Krok 1: Przygotowanie Å›rodowiska

```bash
# Terminal 1: Przygotuj workspace
cd ~/ros2_ws
source /opt/ros/humble/setup.bash
source install/setup.bash

# SprawdÅº czy pakiet jest widoczny
ros2 pkg list | grep g1_pick_and_handover
```

**Oczekiwany rezultat**: `g1_pick_and_handover` pojawia siÄ™ na liÅ›cie.

### Krok 2: Sprawdzenie struktury pakietu

```bash
# Zobacz pliki w pakiecie
ros2 pkg prefix g1_pick_and_handover

# Zobacz dostÄ™pne executable (node'y)
ros2 pkg executables g1_pick_and_handover
```

**Co widzisz?**
- `object_detector` - detekcja obiektÃ³w
- `pose_estimator_6d` - estymacja pozy 3D
- `human_hand_detector` - detekcja dÅ‚oni
- `execute_grasp` - chwytanie
- `execute_handover_wma` - przekazywanie

### Krok 3: Uruchomienie kamery (symulacja)

JeÅ›li nie masz fizycznej kamery, uÅ¼yj danych testowych:

```bash
# Terminal 1: Symuluj topiki kamery
ros2 run image_publisher image_publisher_node \
    test_image.jpg \
    --ros-args \
    -r image_raw:=/camera/color/image_raw

# Terminal 2: SprawdÅº czy topic istnieje
ros2 topic list | grep camera
ros2 topic echo /camera/color/image_raw --once
```

### Krok 4: Uruchomienie detektora obiektÃ³w

```bash
# Terminal 2: Uruchom object detector
ros2 run g1_pick_and_handover object_detector

# Terminal 3: Monitoruj detekcje
ros2 topic echo /object_detections
```

**Zadanie**: PoÅ‚Ã³Å¼ rÃ³Å¼ne obiekty przed kamerÄ… i obserwuj detekcje.

### Krok 5: Wizualizacja w RViz

```bash
# Terminal 4: Uruchom RViz
rviz2
```

**W RViz**:
1. Dodaj display: `Image` â†’ Topic: `/camera/color/image_raw`
2. Dodaj display: `TF` â†’ Zobacz transformacje
3. Dodaj display: `PoseStamped` â†’ Topic: `/object_pose`

**Co siÄ™ nauczyÅ‚eÅ›?**
- âœ… Jak uruchomiÄ‡ node ROS 2
- âœ… Jak monitorowaÄ‡ topiki
- âœ… Jak wizualizowaÄ‡ dane w RViz
- âœ… Podstawowy przepÅ‚yw danych w systemie

---

## ğŸ¥ Tutorial 2: Zrozumienie Percepcji Wizyjnej

**Cel**: ZrozumieÄ‡ jak system "widzi" Å›wiat.

**Czas**: 45 minut

### Krok 1: Detekcja obiektÃ³w z YOLOv5

```bash
# Uruchom detektor z rÃ³Å¼nymi modelami
ros2 run g1_pick_and_handover object_detector \
    --ros-args -p model_name:=yolov5s  # Szybki, mniej dokÅ‚adny

ros2 run g1_pick_and_handover object_detector \
    --ros-args -p model_name:=yolov5m  # Åšredni

ros2 run g1_pick_and_handover object_detector \
    --ros-args -p model_name:=yolov5l  # Wolny, bardzo dokÅ‚adny
```

**Eksperyment 1**: Testuj rÃ³Å¼ne obiekty
- Kubek â˜•
- Telefon ğŸ“±
- KsiÄ…Å¼ka ğŸ“–
- Laptop ğŸ’»

**Pytania do przemyÅ›lenia**:
1. KtÃ³re obiekty sÄ… wykrywane najlepiej?
2. Jak oÅ›wietlenie wpÅ‚ywa na detekcjÄ™?
3. Czy model rozrÃ³Å¼nia podobne obiekty (kubek vs butelka)?

### Krok 2: Estymacja pozy 3D

```bash
# Terminal 1: Detektor obiektÃ³w
ros2 run g1_pick_and_handover object_detector

# Terminal 2: Estymator pozy 6D
ros2 run g1_pick_and_handover pose_estimator_6d

# Terminal 3: Monitoruj pozycje 3D
ros2 topic echo /object_pose
```

**Eksperyment 2**: Zmierz dokÅ‚adnoÅ›Ä‡
1. PoÅ‚Ã³Å¼ obiekt w znanej odlegÅ‚oÅ›ci (np. 50 cm od kamery)
2. Zobacz co zwraca `/object_pose`
3. PorÃ³wnaj z rzeczywistÄ… odlegÅ‚oÅ›ciÄ…

**Zapisz wyniki**:
```bash
# Nagraj dane
ros2 bag record /camera/color/image_raw /object_pose -o experiment_distance
```

### Krok 3: Zrozumienie Pinhole Camera Model

**Teoria**:
```
Konwersja 2D â†’ 3D:
x_3d = (u - cx) * z / fx
y_3d = (v - cy) * z / fy
z_3d = depth[v, u]

gdzie:
- (u, v) = wspÃ³Å‚rzÄ™dne piksela
- (cx, cy) = optyczny Å›rodek kamery
- (fx, fy) = dÅ‚ugoÅ›Ä‡ ogniskowa kamery
- z = gÅ‚Ä™bokoÅ›Ä‡ z sensora RGB-D
```

**Zadanie**: Napisz prosty skrypt testujÄ…cy:

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import PoseStamped

class PoseAnalyzer(Node):
    def __init__(self):
        super().__init__('pose_analyzer')
        self.sub = self.create_subscription(
            PoseStamped, '/object_pose', self.cb, 10)
        
    def cb(self, msg):
        x = msg.pose.position.x
        y = msg.pose.position.y
        z = msg.pose.position.z
        
        distance = (x**2 + y**2 + z**2) ** 0.5
        
        self.get_logger().info(
            f'Object at: ({x:.2f}, {y:.2f}, {z:.2f}), '
            f'distance: {distance:.2f}m'
        )

def main():
    rclpy.init()
    rclpy.spin(PoseAnalyzer())

if __name__ == '__main__':
    main()
```

**Co siÄ™ nauczyÅ‚eÅ›?**
- âœ… Jak dziaÅ‚a detekcja obiektÃ³w z YOLO
- âœ… Jak konwertowaÄ‡ 2D â†’ 3D
- âœ… Jak oceniaÄ‡ dokÅ‚adnoÅ›Ä‡ percepcji
- âœ… Pinhole camera model

---

## ğŸ¦¾ Tutorial 3: Planowanie Ruchu z MoveIt 2

**Cel**: NauczyÄ‡ siÄ™ planowaÄ‡ trajektorie ramienia robota.

**Czas**: 60 minut

### Krok 1: Uruchomienie MoveIt 2 w symulacji

```bash
# Uruchom MoveIt 2 dla robota G1
ros2 launch g1_moveit_config demo.launch.py

# Otworzy siÄ™ RViz z panelem MotionPlanning
```

**W RViz**:
1. W panelu `MotionPlanning`:
   - Planning Group: `arm`
   - PrzeciÄ…gnij interactive marker (zielone strzaÅ‚ki)
2. Kliknij `Plan` aby zaplanowaÄ‡ trajektoriÄ™
3. Kliknij `Execute` aby wykonaÄ‡ (w symulacji)

### Krok 2: Planowanie z kodu Python

StwÃ³rz plik `test_moveit.py`:

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import PoseStamped
from manipulation.moveit_interface import MoveItInterface

class MoveItTester(Node):
    def __init__(self):
        super().__init__('moveit_tester')
        self.moveit = MoveItInterface()
        
        # Czekaj na inicjalizacjÄ™
        self.get_logger().info('MoveIt initialized. Testing...')
        self.test_movements()
    
    def test_movements(self):
        # Test 1: PrzesuÅ„ do pozycji home
        self.get_logger().info('Test 1: Moving to home')
        success = self.moveit.arm.go([0, 0, 0, 0, 0, 0], wait=True)
        self.get_logger().info(f'Result: {"Success" if success else "Failed"}')
        
        # Test 2: PrzesuÅ„ do pozycji testowej
        self.get_logger().info('Test 2: Moving to test pose')
        pose = PoseStamped()
        pose.header.frame_id = "base_link"
        pose.pose.position.x = 0.5
        pose.pose.position.y = 0.0
        pose.pose.position.z = 0.5
        pose.pose.orientation.w = 1.0
        
        success = self.moveit.move_to_pose(pose)
        self.get_logger().info(f'Result: {"Success" if success else "Failed"}')
        
        # Test 3: Sterowanie chwytakiem
        self.get_logger().info('Test 3: Gripper control')
        self.moveit.open_gripper()
        self.get_logger().info('Gripper opened')
        
        import time
        time.sleep(2)
        
        self.moveit.close_gripper()
        self.get_logger().info('Gripper closed')

def main():
    rclpy.init()
    node = MoveItTester()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**Uruchom**:
```bash
chmod +x test_moveit.py
python3 test_moveit.py
```

### Krok 3: Unikanie kolizji

```python
# Dodaj przeszkody do sceny
from manipulation.planning_scene import add_table, add_human

# W metodzie __init__:
add_table(self.moveit.scene)
add_human(self.moveit.scene)

# SprawdÅº w RViz - przeszkody powinny byÄ‡ widoczne
# MoveIt bÄ™dzie teraz unikaÅ‚ kolizji z nimi
```

### Krok 4: Testowanie rÃ³Å¼nych plannerÃ³w

```python
# ZmieÅ„ planner
self.moveit.arm.set_planner_id("RRTConnect")  # DomyÅ›lny, szybki
self.moveit.arm.set_planner_id("RRTstar")     # Wolniejszy, optymalizuje
self.moveit.arm.set_planner_id("PRM")         # Probabilistic Roadmap
self.moveit.arm.set_planner_id("BKPIECE")     # Bidirectional

# Zmierz czas planowania
import time
start = time.time()
success = self.moveit.move_to_pose(pose)
elapsed = time.time() - start
print(f"Planning time: {elapsed:.2f}s")
```

**Zadanie badawcze**:
PorÃ³wnaj plannery:
- RRTConnect vs RRTstar: ktÃ³ry szybszy? ktÃ³ry generuje lepsze trajektorie?
- Jak zmiana `range` (w config/moveit.yaml) wpÅ‚ywa na czas planowania?

**Co siÄ™ nauczyÅ‚eÅ›?**
- âœ… Jak planowaÄ‡ ruch z MoveIt 2
- âœ… Jak dodawaÄ‡ przeszkody
- âœ… RÃ³Å¼nice miÄ™dzy plannerami
- âœ… Interfejs Python do MoveIt 2

---

## ğŸ¤ Tutorial 4: Kompletna Sekwencja Handover

**Cel**: ZrozumieÄ‡ i uruchomiÄ‡ peÅ‚ny system przekazywania obiektÃ³w.

**Czas**: 90 minut

### Krok 1: Przygotowanie systemu

```bash
# Terminal 1: MoveIt 2 (symulacja robota)
ros2 launch g1_moveit_config demo.launch.py

# Terminal 2: RViz (wizualizacja)
rviz2

# Terminal 3: Launch peÅ‚nego pipeline
ros2 launch g1_pick_and_handover full_handover_pipeline.launch.py
```

### Krok 2: Symulacja czÅ‚owieka wyciÄ…gajÄ…cego rÄ™kÄ™

StwÃ³rz plik `simulate_human.py`:

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import PoseStamped
from std_msgs.msg import Bool
import math
import time

class HumanSimulator(Node):
    def __init__(self):
        super().__init__('human_simulator')
        
        # Publishers
        self.pose_pub = self.create_publisher(
            PoseStamped, '/human_hand_pose', 10)
        self.intent_pub = self.create_publisher(
            Bool, '/human_reaching', 10)
        
        # Timer do publikacji
        self.timer = self.create_timer(0.1, self.publish_pose)
        self.t = 0.0
        
    def publish_pose(self):
        # Symuluj ruch dÅ‚oni (sinusoida)
        pose = PoseStamped()
        pose.header.frame_id = "base_link"
        pose.header.stamp = self.get_clock().now().to_msg()
        
        # Pozycja dÅ‚oni (symulacja wyciÄ…gniÄ™cia)
        pose.pose.position.x = 0.6
        pose.pose.position.y = 0.1 * math.sin(self.t)
        pose.pose.position.z = 0.9
        pose.pose.orientation.w = 1.0
        
        self.pose_pub.publish(pose)
        
        # Intencja (wyciÄ…gniÄ™cie rÄ™ki)
        intent = Bool()
        intent.data = True  # CzÅ‚owiek wyciÄ…ga rÄ™kÄ™
        self.intent_pub.publish(intent)
        
        self.t += 0.1
        
        self.get_logger().info(
            f'Publishing hand pose: x={pose.pose.position.x:.2f}, '
            f'y={pose.pose.position.y:.2f}, z={pose.pose.position.z:.2f}'
        )

def main():
    rclpy.init()
    node = HumanSimulator()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**Uruchom**:
```bash
python3 simulate_human.py
```

**Obserwuj**:
- Robot powinien reagowaÄ‡ na pozycjÄ™ dÅ‚oni
- SprawdÅº logi w terminalu z `execute_handover_wma`

### Krok 3: Sekwencja TAKE_FROM_HUMAN

**Scenariusz**: CzÅ‚owiek podaje obiekt robotowi.

```bash
# Terminal dodatkowy: Symuluj obiekt w dÅ‚oni
ros2 topic pub /object_pose geometry_msgs/PoseStamped "{
  header: {frame_id: 'base_link'},
  pose: {
    position: {x: 0.6, y: 0.0, z: 0.9},
    orientation: {w: 1.0}
  }
}" --once

# Terminal dodatkowy: CzÅ‚owiek wyciÄ…ga rÄ™kÄ™
ros2 topic pub /human_reaching std_msgs/Bool "data: true" --once
ros2 topic pub /gripper_state std_msgs/Bool "data: false" --once
```

**Oczekiwany przepÅ‚yw**:
1. WMA wykrywa: `human_reaching=True, gripper_occupied=False`
2. Decyzja: `TAKE_FROM_HUMAN`
3. Robot:
   - Oblicza pozycjÄ™ handover
   - Planuje trajektoriÄ™
   - Wykonuje ruch
   - Zamyka chwytak

### Krok 4: Sekwencja GIVE_TO_HUMAN

**Scenariusz**: Robot przekazuje obiekt czÅ‚owiekowi.

```bash
# Robot trzyma obiekt
ros2 topic pub /gripper_state std_msgs/Bool "data: true" --once

# CzÅ‚owiek wyciÄ…ga rÄ™kÄ™
ros2 topic pub /human_reaching std_msgs/Bool "data: true" --once
```

**Oczekiwany przepÅ‚yw**:
1. WMA wykrywa: `human_reaching=True, gripper_occupied=True`
2. Decyzja: `GIVE_TO_HUMAN`
3. Robot:
   - Oblicza pozycjÄ™ handover
   - Przesuwa obiekt do dÅ‚oni czÅ‚owieka
   - Otwiera chwytak

### Krok 5: Analiza automatu stanÃ³w

**Narysuj diagram**:
```
Stan poczÄ…tkowy: IDLE
  |
  | (human_reaching && !gripper_occupied)
  v
TAKE_FROM_HUMAN
  |
  | (grasp successful)
  v
HOLD
  |
  | (human_reaching && gripper_occupied)
  v
GIVE_TO_HUMAN
  |
  | (object released)
  v
IDLE
```

**Zadanie**: Zmodyfikuj logikÄ™ decyzyjnÄ…
```python
# W execute_handover.py, metoda mock_decision():
def mock_decision(self) -> str:
    # Dodaj wiÄ™cej warunkÃ³w
    if self.human_reaching:
        # SprawdÅº odlegÅ‚oÅ›Ä‡
        distance = self.calculate_distance(self.human_pose)
        
        if distance < 0.3:  # Zbyt blisko
            return ACTION_IDLE
        
        if not self.gripper_occupied:
            return ACTION_TAKE_FROM_HUMAN
        else:
            return ACTION_GIVE_TO_HUMAN
    return ACTION_IDLE
```

**Co siÄ™ nauczyÅ‚eÅ›?**
- âœ… PeÅ‚ny przepÅ‚yw systemu handover
- âœ… Integracja percepcji, decyzji i manipulacji
- âœ… Automat stanÃ³w (FSM)
- âœ… Symulacja interakcji czÅ‚owiek-robot

---

## ğŸ§ª Tutorial 5: Eksperymentowanie z Parametrami

**Cel**: ZoptymalizowaÄ‡ parametry systemu dla rÃ³Å¼nych scenariuszy.

**Czas**: 60 minut

### Eksperyment 1: Optymalizacja Chwytania

**Pytanie badawcze**: Jak `approach_distance` wpÅ‚ywa na success rate?

```bash
# Test 1: approach_distance = 0.05m
# Edytuj config/grasp_params.yaml
approach_distance: 0.05

# Test 2: approach_distance = 0.10m (domyÅ›lne)
approach_distance: 0.10

# Test 3: approach_distance = 0.15m
approach_distance: 0.15

# Dla kaÅ¼dego: wykonaj 10 prÃ³b chwytania, zapisz success rate
```

**Tabela wynikÃ³w**:
```
| Offset | Sukces | Czas [s] | Uwagi          |
|--------|--------|----------|----------------|
| 0.05m  | 6/10   | 3.2      | Uderza w stÃ³Å‚  |
| 0.10m  | 9/10   | 4.1      | Optymalne      |
| 0.15m  | 8/10   | 5.5      | Wolne          |
```

### Eksperyment 2: Planery MoveIt

**Pytanie badawcze**: KtÃ³ry planner jest najlepszy dla handover?

```python
planners = ["RRTConnect", "RRTstar", "PRM", "BKPIECE"]

for planner in planners:
    self.arm.set_planner_id(planner)
    
    # Zmierz czas i success rate
    start = time.time()
    success = self.move_to_pose(target_pose)
    elapsed = time.time() - start
    
    print(f"{planner}: {elapsed:.2f}s, {'OK' if success else 'FAIL'}")
```

### Eksperyment 3: PrÃ³g PewnoÅ›ci YOLOv5

**Pytanie badawcze**: Jaki `confidence_threshold` minimalizuje false positives?

```bash
# Test z rÃ³Å¼nymi progami
for threshold in 0.3 0.4 0.5 0.6 0.7 0.8 0.9
do
    echo "Testing threshold: $threshold"
    ros2 run g1_pick_and_handover object_detector \
        --ros-args -p confidence_threshold:=$threshold
    
    # Policz detekcje w 60 sekundach
    timeout 60 ros2 topic echo /object_detections | grep -c "detections"
done
```

**Wnioski**:
- Niski prÃ³g (0.3): Wiele detekcji, ale teÅ¼ false positives
- Wysoki prÃ³g (0.8): Mniej detekcji, ale pewniejsze
- Optimal: 0.6 (kompromis)

**Co siÄ™ nauczyÅ‚eÅ›?**
- âœ… Metodologia eksperymentÃ³w robotycznych
- âœ… Analiza wydajnoÅ›ci systemu
- âœ… Optymalizacja parametrÃ³w
- âœ… Trade-offs (szybkoÅ›Ä‡ vs dokÅ‚adnoÅ›Ä‡)

---

## ğŸ“ Zadania Projektowe

### Projekt 1: Custom Object Detector (Åatwy)

**Cel**: DodaÄ‡ detekcjÄ™ wÅ‚asnej klasy obiektÃ³w.

**Kroki**:
1. Zbierz dataset (50+ zdjÄ™Ä‡)
2. Adnotuj dane (labelImg, Roboflow)
3. Wytrenuj YOLOv5:
   ```bash
   python train.py --data custom.yaml --weights yolov5s.pt --epochs 50
   ```
4. ZaÅ‚aduj w object_detector.py:
   ```python
   self.model = torch.hub.load('ultralytics/yolov5', 'custom', path='best.pt')
   ```

### Projekt 2: Adaptive Grasping (Åšredni)

**Cel**: Robot dostosowuje strategiÄ™ chwytania do wielkoÅ›ci obiektu.

**PomysÅ‚**:
```python
def compute_adaptive_grasp(object_size):
    if object_size < 0.05:  # MaÅ‚y obiekt
        return {"approach": 0.08, "speed": 0.2}
    elif object_size < 0.15:  # Åšredni obiekt
        return {"approach": 0.10, "speed": 0.3}
    else:  # DuÅ¼y obiekt
        return {"approach": 0.15, "speed": 0.4}
```

### Projekt 3: Collision-Free Path Planning (Trudny)

**Cel**: Robot aktywnie unika ruchomych przeszkÃ³d.

**PomysÅ‚**:
- CiÄ…gÅ‚e aktualizowanie planning scene
- Monitorowanie czujnikÃ³w odlegÅ‚oÅ›ci
- Real-time replanning jeÅ›li wykryto przeszkodÄ™

---

**Powodzenia w nauce!** ğŸš€

JeÅ›li masz pytania lub napotkasz problemy, zajrzyj do [FAQ.md](FAQ.md) lub otwÃ³rz Issue na GitHubie.
