# Robot G1 - System Przekazywania ObiektÃ³w (Handover)

## ğŸ“– Wprowadzenie

Ten projekt demonstruje system interakcji czÅ‚owiek-robot dla robota humanoidalnego Unitree G1. System umoÅ¼liwia robotowi:
- **Odbieranie obiektÃ³w** od czÅ‚owieka (TAKE_FROM_HUMAN)
- **Przekazywanie obiektÃ³w** czÅ‚owiekowi (GIVE_TO_HUMAN)
- Wykorzystanie **World Model AI (WMA)** do podejmowania inteligentnych decyzji

Projekt zostaÅ‚ stworzony w celach edukacyjnych, aby pomÃ³c studentom zrozumieÄ‡:
- Programowanie robotÃ³w humanoidalnych w ROS 2
- PercepcjÄ™ wizyjnÄ… (wykrywanie obiektÃ³w i dÅ‚oni czÅ‚owieka)
- Planowanie ruchu ramienia robota z uÅ¼yciem MoveIt 2
- IntegracjÄ™ sztucznej inteligencji (WMA) z systemami robotycznymi

## ğŸ¯ Architektura Systemu

System skÅ‚ada siÄ™ z czterech gÅ‚Ã³wnych moduÅ‚Ã³w:

### 1. ğŸ“¸ **Perception (Percepcja)**
Odpowiada za rozumienie otoczenia robota poprzez kamery i czujniki.

**Pliki:**
- `human_hand_detector.py` - Wykrywa pozycjÄ™ dÅ‚oni czÅ‚owieka w przestrzeni 3D
- `object_detector.py` - Wykrywa obiekty na obrazie z kamery (uÅ¼ywa YOLOv5)
- `pose_estimator_6d.py` - Oblicza pozÄ™ 6D obiektÃ³w (pozycja x,y,z + orientacja)
- `static_tf_camera.py` - Definiuje transformacjÄ™ kamery wzglÄ™dem bazy robota

**Jak to dziaÅ‚a:**
1. Kamera RGB-D rejestruje obraz kolorowy i mapÄ™ gÅ‚Ä™bokoÅ›ci
2. YOLOv5 wykrywa obiekty na obrazie RGB
3. UÅ¼ywajÄ…c mapy gÅ‚Ä™bokoÅ›ci, system oblicza pozycjÄ™ 3D obiektu
4. MediaPipe/OpenPose wykrywa kluczowe punkty dÅ‚oni czÅ‚owieka

### 2. ğŸ¦¾ **Manipulation (Manipulacja)**
Odpowiada za planowanie i wykonywanie ruchÃ³w ramienia robota.

**Pliki:**
- `moveit_interface.py` - Interface do MoveIt 2 (planowanie trajektorii)
- `handover_planner.py` - Oblicza pozycjÄ™ do przekazania obiektu
- `grasp_planner.py` - Oblicza pozycjÄ™ pre-grasp przedchwyceniem
- `execute_grasp.py` - Wykonuje sekwencjÄ™ chwytania obiektu
- `execute_handover.py` - Wykonuje sekwencjÄ™ przekazywania obiektu
- `planning_scene.py` - Dodaje przeszkody (stÃ³Å‚, czÅ‚owiek) do sceny planowania

**Jak to dziaÅ‚a:**
1. MoveIt 2 planuje trajektoriÄ™ ruchu ramienia bez kolizji
2. System oblicza pozycje poÅ›rednie (pre-grasp, grasp, handover)
3. Gripper (chwytak) otwiera siÄ™ i zamyka w odpowiednich momentach
4. Robot wykonuje ruch pÅ‚ynnie i bezpiecznie

### 3. ğŸ§  **Decision (Decyzje)**
Wykorzystuje sztucznÄ… inteligencjÄ™ do podejmowania decyzji o akcjach robota.

**Pliki:**
- `wma_handover_manager.py` - Manager decyzji WMA dla przekazywania obiektÃ³w
- `wma_task_manager.py` - Manager stanÃ³w automatu skoÅ„czonego (FSM)

**Jak to dziaÅ‚a:**
1. World Model AI (WMA) analizuje obserwacje z kamer i czujnikÃ³w
2. WMA przewiduje intencje czÅ‚owieka (czy chce daÄ‡/wziÄ…Ä‡ obiekt)
3. System podejmuje decyzjÄ™: TAKE_FROM_HUMAN, GIVE_TO_HUMAN lub IDLE
4. FSM (Finite State Machine) zarzÄ…dza przejÅ›ciami miÄ™dzy stanami

### 4. ğŸš€ **Launch**
Pliki uruchomieniowe, ktÃ³re startujÄ… wszystkie komponenty systemu.

**Pliki:**
- `full_pipeline.launch.py` - Uruchamia podstawowy pipeline
- `full_handover_pipeline.launch.py` - Uruchamia kompletny system handover

## ğŸ”§ Instalacja

### Wymagania wstÄ™pne

1. **Ubuntu 22.04 LTS** (zalecane)
2. **ROS 2 Humble** lub nowszy
3. **Python 3.10+**
4. **MoveIt 2** (dla planowania ruchu)
5. **Kamera RGB-D** (np. Intel RealSense D435)

### Krok 1: Instalacja ROS 2

```bash
# Dodaj repozytorium ROS 2
sudo apt update && sudo apt install software-properties-common
sudo add-apt-repository universe
sudo apt update && sudo apt install curl -y
sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg

# Dodaj ÅºrÃ³dÅ‚a ROS 2
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null

# Zainstaluj ROS 2 Humble
sudo apt update
sudo apt install ros-humble-desktop
```

### Krok 2: Instalacja MoveIt 2

```bash
sudo apt install ros-humble-moveit
```

### Krok 3: Instalacja zaleÅ¼noÅ›ci Python

```bash
# Zainstaluj pip jeÅ›li nie masz
sudo apt install python3-pip

# Zainstaluj wymagane pakiety z pliku requirements.txt
pip3 install -r requirements.txt
```

### Krok 4: Klonowanie i konfiguracja workspace

```bash
# UtwÃ³rz workspace ROS 2
mkdir -p ~/ros2_ws/src
cd ~/ros2_ws/src

# Sklonuj to repozytorium
git clone https://github.com/MatPomGit/robot-g1-Handover.git

# Zainstaluj zaleÅ¼noÅ›ci ROS 2
cd ~/ros2_ws
rosdep install --from-paths src --ignore-src -r -y

# Zbuduj workspace
colcon build
source install/setup.bash
```

### Krok 5: Konfiguracja modelu robota G1

```bash
# Pobierz opis robota Unitree G1
cd ~/ros2_ws/src
git clone https://github.com/unitreerobotics/unitree_ros.git
cd ~/ros2_ws
colcon build --packages-select g1_description
```

## ğŸ® UÅ¼ycie

### Uruchomienie peÅ‚nego systemu

```bash
# W pierwszym terminalu - uruchom ÅºrÃ³dÅ‚o workspace
cd ~/ros2_ws
source install/setup.bash

# Uruchom kompletny pipeline handover
ros2 launch g1_pick_and_handover full_handover_pipeline.launch.py
```

### Uruchomienie poszczegÃ³lnych komponentÃ³w

#### 1. Tylko percepcja (wykrywanie obiektÃ³w i dÅ‚oni)

```bash
# Terminal 1: Detekcja obiektÃ³w
ros2 run g1_pick_and_handover object_detector

# Terminal 2: Estymacja pozy 6D
ros2 run g1_pick_and_handover pose_estimator_6d

# Terminal 3: Detekcja dÅ‚oni czÅ‚owieka
ros2 run g1_pick_and_handover human_hand_detector
```

#### 2. Tylko manipulacja (test ruchu ramienia)

```bash
# Uruchom interfejs MoveIt
ros2 run g1_pick_and_handover moveit_interface
```

#### 3. System decyzyjny z WMA

```bash
ros2 run g1_pick_and_handover execute_handover_wma
```

## ğŸ“Š PrzepÅ‚yw Danych (Topics ROS 2)

System komunikuje siÄ™ poprzez nastÄ™pujÄ…ce topiki ROS 2:

| Topic | Typ | Opis |
|-------|-----|------|
| `/camera/color/image_raw` | `sensor_msgs/Image` | Obraz RGB z kamery |
| `/camera/depth/image_raw` | `sensor_msgs/Image` | Mapa gÅ‚Ä™bokoÅ›ci z kamery |
| `/camera/color/camera_info` | `sensor_msgs/CameraInfo` | Kalibracja kamery |
| `/object_detections` | `vision_msgs/Detection2DArray` | Wykryte obiekty (2D bounding boxes) |
| `/object_pose` | `geometry_msgs/PoseStamped` | Pozycja 3D obiektu |
| `/human_hand_pose` | `geometry_msgs/PoseStamped` | Pozycja dÅ‚oni czÅ‚owieka |
| `/human_reaching` | `std_msgs/Bool` | Czy czÅ‚owiek wyciÄ…ga rÄ™kÄ™ |
| `/gripper_state` | `std_msgs/Bool` | Stan chwytaka (otwarty/zamkniÄ™ty) |

## ğŸ”„ Automat StanÃ³w (FSM)

System dziaÅ‚a wedÅ‚ug nastÄ™pujÄ…cego automatu stanÃ³w:

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     IDLE      â”‚  â† Stan poczÄ…tkowy
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
    human_reaching=True
    gripper_occupied=False
                â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  TAKE_FROM_HUMAN   â”‚  â† Robot podjeÅ¼dÅ¼a do dÅ‚oni czÅ‚owieka
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
gripper_closed=True
            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     HOLD      â”‚  â† Robot trzyma obiekt
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
human_reaching=True
gripper_occupied=True
                â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  GIVE_TO_HUMAN     â”‚  â† Robot przekazuje obiekt czÅ‚owiekowi
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
gripper_open=True
            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     IDLE      â”‚  â† PowrÃ³t do stanu poczÄ…tkowego
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Tutoriale dla StudentÃ³w

### Tutorial 1: Zrozumienie percepcji wizyjnej

1. Uruchom detektor obiektÃ³w:
   ```bash
   ros2 run g1_pick_and_handover object_detector
   ```

2. Obserwuj wykryte obiekty:
   ```bash
   ros2 topic echo /object_detections
   ```

3. **Zadanie**: PoÅ‚Ã³Å¼ rÃ³Å¼ne obiekty przed kamerÄ… i obserwuj, jak system je wykrywa.

### Tutorial 2: Planowanie ruchu z MoveIt 2

1. Uruchom MoveIt 2 w trybie wizualizacji:
   ```bash
   ros2 launch moveit2_tutorials demo.launch.py
   ```

2. **Zadanie**: UÅ¼ywajÄ…c RViz, zaplanuj trajektoriÄ™ ruchu ramienia do rÃ³Å¼nych pozycji.

### Tutorial 3: Integracja WMA

1. Przestudiuj plik `wma_handover_manager.py`
2. Zrozum, jak obserwacje sÄ… przeksztaÅ‚cane w tensory PyTorch
3. **Zadanie**: Dodaj nowÄ… obserwacjÄ™ (np. kolor obiektu) do systemu WMA

## ğŸ“ Struktura KatalogÃ³w

```
robot-g1-Handover/
â”œâ”€â”€ README.md                    # Ten plik
â”œâ”€â”€ CONTRIBUTING.md              # Przewodnik dla kontrybutorÃ³w
â”œâ”€â”€ requirements.txt             # ZaleÅ¼noÅ›ci Python
â”œâ”€â”€ package.xml                  # Deskryptor pakietu ROS 2
â”œâ”€â”€ setup.py                     # Konfiguracja instalacji Python
â”œâ”€â”€ setup.cfg                    # Konfiguracja setuptools
â”œâ”€â”€ perception/                  # ModuÅ‚ percepcji
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ human_hand_detector.py
â”‚   â”œâ”€â”€ object_detector.py
â”‚   â”œâ”€â”€ pose_estimator_6d.py
â”‚   â””â”€â”€ static_tf_camera.py
â”œâ”€â”€ manipulation/                # ModuÅ‚ manipulacji
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ moveit_interface.py
â”‚   â”œâ”€â”€ handover_planner.py
â”‚   â”œâ”€â”€ grasp_planner.py
â”‚   â”œâ”€â”€ execute_grasp.py
â”‚   â”œâ”€â”€ execute_handover.py
â”‚   â””â”€â”€ planning_scene.py
â”œâ”€â”€ decision/                    # ModuÅ‚ decyzyjny
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ wma_handover_manager.py
â”‚   â””â”€â”€ wma_task_manager.py
â”œâ”€â”€ launch/                      # Pliki uruchomieniowe
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ full_pipeline.launch.py
â”‚   â”œâ”€â”€ full_handover_pipeline.launch.py
â”‚   â””â”€â”€ launch_perception.launch.py
â””â”€â”€ config/                      # Pliki konfiguracyjne
    â”œâ”€â”€ README.md
    â”œâ”€â”€ grasp_params.yaml
    â””â”€â”€ moveit.yaml
```

## ğŸ” Wizualizacja z MuJoCo

Aby zwizualizowaÄ‡ robota G1 w symulatorze:

```bash
# Zainstaluj MuJoCo
pip install mujoco

# Uruchom viewer
python -m mujoco.viewer

# PrzeciÄ…gnij plik MJCF/URDF modelu G1 do okna viewera
# Pliki modeli: https://github.com/unitreerobotics/unitree_ros/tree/master/robots/g1_description
```

## ğŸ› ï¸ Konfiguracja

### Parametry chwytania (grasp_params.yaml)

- `approach_distance`: OdlegÅ‚oÅ›Ä‡ podejÅ›cia przed chwyceniem (0.10m)
- `lift_distance`: WysokoÅ›Ä‡ podniesienia pochwyceniu (0.15m)
- `gripper_open`: Otwarcie chwytaka (0.04m)
- `gripper_closed`: ZamkniÄ™cie chwytaka (0.0m)
- `max_force`: Maksymalna siÅ‚a chwytaka (30.0N)

### Parametry MoveIt 2 (moveit.yaml)

- Planner: RRTConnect (algorytm planowania trajektorii)
- Range: 0.3 (zakres prÃ³bkowania dla RRT)

## ğŸ› RozwiÄ…zywanie problemÃ³w

### Problem: Kamera nie jest wykrywana

```bash
# SprawdÅº, czy kamera jest podÅ‚Ä…czona
rs-enumerate-devices

# Uruchom node kamery RealSense
ros2 run realsense2_camera realsense2_camera_node
```

### Problem: MoveIt 2 nie planuje trajektorii

1. SprawdÅº, czy robot jest poprawnie zdefiniowany w URDF
2. Upewnij siÄ™, Å¼e scena planowania nie zawiera kolizji
3. ZwiÄ™ksz timeout planowania

### Problem: WMA nie dziaÅ‚a

1. Upewnij siÄ™, Å¼e masz zainstalowany PyTorch z CUDA (jeÅ›li uÅ¼ywasz GPU)
2. SprawdÅº Å›cieÅ¼kÄ™ do checkpointu WMA
3. SprawdÅº, czy obserwacje majÄ… poprawny format

## ğŸ“š Dokumentacja Projektu

### ğŸ“– Podstawowa Dokumentacja
- **[FAQ.md](FAQ.md)** - NajczÄ™Å›ciej zadawane pytania i rozwiÄ…zywanie problemÃ³w
- **[TUTORIALS.md](TUTORIALS.md)** - SzczegÃ³Å‚owe tutoriale krok po kroku dla studentÃ³w
- **[GLOSSARY.md](GLOSSARY.md)** - SÅ‚ownik terminÃ³w i konceptÃ³w
- **[QUICK_REFERENCE.md](QUICK_REFERENCE.md)** - Szybka Å›ciÄ…ga z komendami i parametrami

### ğŸ—ï¸ Dokumentacja Techniczna
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Architektura systemu i przepÅ‚yw danych
- **[TESTING.md](TESTING.md)** - Strategia i implementacja testÃ³w
- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Przewodnik dla kontrybutorÃ³w

### ğŸ“ Dokumentacja ModuÅ‚Ã³w
- **[perception/README.md](perception/README.md)** - ModuÅ‚ percepcji (kamery, detekcja)
- **[manipulation/README.md](manipulation/README.md)** - ModuÅ‚ manipulacji (MoveIt, grasp)
- **[decision/README.md](decision/README.md)** - ModuÅ‚ decyzyjny (WMA, FSM)
- **[launch/README.md](launch/README.md)** - Pliki uruchomieniowe
- **[config/README.md](config/README.md)** - Pliki konfiguracyjne

## ğŸŒ ZewnÄ™trzne Zasoby

- [ROS 2 Documentation](https://docs.ros.org/en/humble/)
- [MoveIt 2 Tutorials](https://moveit.picknik.ai/humble/index.html)
- [Unitree G1 Robot](https://www.unitree.com/g1)
- [World Model AI](https://worldmodels.github.io/)
- [YOLOv5 Documentation](https://github.com/ultralytics/yolov5)

## ğŸ‘¥ Autorzy i Licencja

Ten projekt jest open-source i dostÄ™pny do celÃ³w edukacyjnych.

## ğŸ¤ WspÃ³Å‚praca

JeÅ›li masz pytania lub sugestie, otwÃ³rz Issue lub Pull Request na GitHubie!

---

**Uwaga**: Ten projekt jest w fazie rozwoju i sÅ‚uÅ¼y celom edukacyjnym. Przed uÅ¼yciem na prawdziwym robocie naleÅ¼y dokÅ‚adnie przetestowaÄ‡ wszystkie funkcje w symulacji.
