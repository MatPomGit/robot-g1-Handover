# ModuÅ‚ Decyzyjny (Decision Module)

## ðŸ“– Wprowadzenie

ModuÅ‚ decyzyjny wykorzystuje sztucznÄ… inteligencjÄ™ (World Model AI - WMA) do podejmowania inteligentnych decyzji o akcjach robota. Jest to "mÃ³zg" robota, ktÃ³ry analizuje sytuacjÄ™ i wybiera najlepsze dziaÅ‚anie.

## ðŸŽ¯ Funkcje ModuÅ‚u

### 1. **WMA Handover Manager** (`wma_handover_manager.py`)
- Podejmuje decyzje o akcjach handover
- Analizuje multimodalne obserwacje
- Przewiduje intencje czÅ‚owieka
- Planuje sekwencje akcji

### 2. **WMA Task Manager** (`wma_task_manager.py`)
- ZarzÄ…dza automatem stanÃ³w (FSM)
- Sekwencja chwytania: idle -> approach -> grasp -> lift
- Uproszczony model dydaktyczny

## ðŸ§  Co to jest World Model AI (WMA)?

World Model AI to zaawansowany model sztucznej inteligencji, ktÃ³ry:

### Architektura:
```
Obserwacje (RGB, gÅ‚Ä™bokoÅ›Ä‡, stan)
        â†“
    Encoder (CNN, Transformer)
        â†“
Reprezentacja Latentna (compressed state)
        â†“
  World Model (przewiduje przyszÅ‚oÅ›Ä‡)
        â†“
    Policy (wybiera akcje)
        â†“
   Akcje (TAKE/GIVE/IDLE)
```

### Kluczowe Cechy:
1. **Multimodalne**: Przetwarza obraz, czujniki, stan
2. **Predykcyjne**: Przewiduje co siÄ™ stanie po akcji
3. **Hierarchiczne**: Planuje na rÃ³Å¼nych poziomach abstrakcji
4. **Adaptacyjne**: Uczy siÄ™ z doÅ›wiadczenia

## ðŸ“Š PrzepÅ‚yw Decyzji

```
Obserwacje:
â”œâ”€â”€ camera_rgb (obraz z kamery)
â”œâ”€â”€ gripper_state (stan chwytaka)
â”œâ”€â”€ human_pose (pozycja dÅ‚oni)
â””â”€â”€ human_reaching (intencja)
        â†“
    [WMA Preprocessing]
        â†“
  Tensory PyTorch:
  â”œâ”€â”€ camera_tensor [1, 3, H, W]
  â”œâ”€â”€ binary_features [1, 2]
  â””â”€â”€ human_pos [1, 3]
        â†“
    [WMA Inference]
        â†“
Akcje: ['IDLE', 'TAKE_FROM_HUMAN', 'GIVE_TO_HUMAN']
        â†“
    [WybÃ³r pierwszej akcji]
        â†“
    Wykonanie (MoveIt 2)
```

## ðŸ”§ Decyzje WMA

### TAKE_FROM_HUMAN
**Kiedy:** CzÅ‚owiek trzyma obiekt i wyciÄ…ga rÄ™kÄ™ w kierunku robota

**Sekwencja:**
1. Oblicz pozycjÄ™ handover (z offsetem)
2. PrzesuÅ„ ramiÄ™ do pozycji
3. Zamknij chwytak (chwyÄ‡ obiekt)

**Obserwacje wskazujÄ…ce na TAKE:**
- human_reaching = True
- gripper_state = False (pusty)
- CzÅ‚owiek trzyma obiekt (z obrazu)
- DÅ‚oÅ„ blisko robota

### GIVE_TO_HUMAN
**Kiedy:** Robot trzyma obiekt, a czÅ‚owiek wyciÄ…ga rÄ™kÄ™

**Sekwencja:**
1. Oblicz pozycjÄ™ handover
2. PrzesuÅ„ ramiÄ™ (z obiektem) do pozycji
3. OtwÃ³rz chwytak (puÅ›Ä‡ obiekt)

**Obserwacje wskazujÄ…ce na GIVE:**
- human_reaching = True
- gripper_state = True (trzyma obiekt)
- DÅ‚oÅ„ blisko robota
- Otwarta dÅ‚oÅ„ (z obrazu)

### IDLE
**Kiedy:** Brak jasnej intencji lub sytuacja niejednoznaczna

**Zachowanie:**
- Robot czeka
- Nie wykonuje ruchu
- Monitoruje sytuacjÄ™

**Obserwacje wskazujÄ…ce na IDLE:**
- human_reaching = False
- DÅ‚oÅ„ daleko od robota
- Niejednoznaczna gestykulacja

## ðŸš€ UÅ¼ycie

### Uruchomienie WMA Handover:

```bash
# Terminal 1: MoveIt 2
ros2 launch g1_moveit_config demo.launch.py

# Terminal 2: WMA Handover Manager
ros2 run g1_pick_and_handover execute_handover_wma

# Terminal 3: Publikuj obserwacje (test)
ros2 topic pub /human_hand_pose geometry_msgs/PoseStamped "{...}"
ros2 topic pub /human_reaching std_msgs/Bool "data: true"
```

### Testowanie decyzji:

```python
# Python REPL
import rclpy
from decision.wma_handover_manager import WMAHandoverManager

wma = WMAHandoverManager()

observation = {
    "camera_rgb": image,         # numpy array
    "gripper_state": False,
    "human_pose": pose,
    "human_reaching": True
}

action = wma.infer_action(observation)
print(f"WMA Decision: {action}")  # 'TAKE_FROM_HUMAN'
```

## ðŸ“š Kluczowe Algorytmy

### Model Predictive Control (MPC)
WMA uÅ¼ywa MPC do planowania:

```
Horizon = 8 krokÃ³w
t=0: IDLE
t=1: IDLE
t=2: TAKE_FROM_HUMAN â† wykonaj teraz
t=3: IDLE
t=4: IDLE
t=5: GIVE_TO_HUMAN
t=6: IDLE
t=7: IDLE
```

### Preprocessing Obserwacji

```python
# Obraz RGB
camera_tensor = torch.from_numpy(camera)
camera_tensor = camera_tensor.permute(2, 0, 1)  # HWC -> CHW
camera_tensor = camera_tensor.float() / 255.0   # [0-255] -> [0-1]
camera_tensor = camera_tensor.unsqueeze(0)      # [C,H,W] -> [1,C,H,W]

# Binary features
binary = torch.tensor([gripper_state, human_reaching])
binary = binary.float().unsqueeze(0)            # [2] -> [1, 2]

# Pozycja czÅ‚owieka
human_pos = torch.tensor([x, y, z])
human_pos = human_pos.float().unsqueeze(0)      # [3] -> [1, 3]
```

### Automat StanÃ³w (FSM) - Uproszczony

```
    idle
     â†“
  approach (pre-grasp)
     â†“
   grasp (chwyt)
     â†“
    lift (podniesienie)
```

**UWAGA**: To uproszczenie dydaktyczne! Prawdziwy WMA uÅ¼ywa probabilistycznych przejÅ›Ä‡.

## ðŸ› ï¸ Konfiguracja

### Checkpoint WMA:

```bash
# Pobierz pretrenowany checkpoint
wget https://example.com/unifolm_wma_checkpoint.pth

# Lub wytrenuj wÅ‚asny
python train_wma.py --dataset handover_data --epochs 100
```

### Parametry WMA:

```python
wma = WMAHandoverManager(
    checkpoint_path="/path/to/checkpoint",
    device="cuda",           # lub "cpu"
    horizon=8,               # Horyzont planowania
    temperature=1.0          # Temperatura sampling
)
```

## ðŸ” Debugowanie

### Problem: WMA nie podejmuje decyzji

```python
# SprawdÅº czy checkpoint istnieje
import os
assert os.path.exists(checkpoint_path)

# SprawdÅº czy obserwacje sÄ… poprawne
print(f"Camera shape: {observation['camera_rgb'].shape}")
print(f"Gripper state: {observation['gripper_state']}")

# WÅ‚Ä…cz verbose logging
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Problem: ZÅ‚e decyzje

Przyczyny:
1. **Checkpoint nie pasuje** - wytrenowany na innych danych
2. **NieprawidÅ‚owy preprocessing** - format tensorÃ³w
3. **Brak danych treningowych** - maÅ‚o przykÅ‚adÃ³w tej sytuacji

RozwiÄ…zania:
```python
# Re-trenuj na wÅ‚asnych danych
# Zbierz wiÄ™cej przykÅ‚adÃ³w problematycznych sytuacji
# UÅ¼yj data augmentation
```

### Problem: Wolny inference

```bash
# UÅ¼yj GPU
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Zmniejsz rozmiar obrazu
camera_resized = cv2.resize(camera, (320, 240))

# UÅ¼yj TensorRT (dla NVIDIA GPU)
# Konwertuj model na TensorRT engine
```

## ðŸ“– Tutorial dla StudentÃ³w

### Ä†wiczenie 1: Analiza Decyzji WMA

1. Uruchom WMA w trybie debug

2. Zbierz log decyzji dla rÃ³Å¼nych scenariuszy:
   - CzÅ‚owiek wyciÄ…ga rÄ™kÄ™ z obiektem
   - CzÅ‚owiek wyciÄ…ga pustÄ… rÄ™kÄ™
   - CzÅ‚owiek daleko od robota

3. **Zadanie**: StwÃ³rz tabelÄ™: Obserwacje -> Decyzja

### Ä†wiczenie 2: Modyfikacja Preprocessing

1. W `wma_handover_manager.py`, dodaj nowÄ… feature:
   ```python
   object_in_hand = detect_object_in_hand(observation['camera_rgb'])
   features = torch.cat([binary_features, object_in_hand], dim=1)
   ```

2. **Zadanie**: SprawdÅº czy to poprawia decyzje

### Ä†wiczenie 3: PorÃ³wnanie z ReguÅ‚ami

1. Zaimplementuj prosty system reguÅ‚:
   ```python
   if human_reaching and not gripper_occupied:
       return "TAKE_FROM_HUMAN"
   elif human_reaching and gripper_occupied:
       return "GIVE_TO_HUMAN"
   else:
       return "IDLE"
   ```

2. **Zadanie**: PorÃ³wnaj success rate: WMA vs ReguÅ‚y

### Ä†wiczenie 4: Trenowanie WMA

1. Zbierz dataset handover (100+ przykÅ‚adÃ³w)

2. Format danych:
   ```
   dataset/
   â”œâ”€â”€ episode_0001/
   â”‚   â”œâ”€â”€ obs_0.png (obraz)
   â”‚   â”œâ”€â”€ state_0.json (stan)
   â”‚   â””â”€â”€ action_0.txt (akcja)
   â”œâ”€â”€ episode_0002/
   ...
   ```

3. **Zadanie**: Wytrenuj wÅ‚asny WMA checkpoint

## ðŸ”¬ Zaawansowane

### Hierarchical WMA

```python
# High-level policy
high_action = wma_high.infer("handover_task")  # "approach"

# Low-level policy
if high_action == "approach":
    low_actions = wma_low.plan_trajectory(
        start=current_pose,
        goal=target_pose
    )
```

### Multi-Task WMA

```python
# Jeden WMA dla wielu zadaÅ„
wma = WorldModelPolicy.from_pretrained("multi_task_checkpoint")

action = wma.infer_action(
    observation=obs,
    task="handover"  # lub "pick", "place", "clean"
)
```

### Online Learning

```python
# Robot uczy siÄ™ podczas pracy
experience = (observation, action, reward, next_observation)
wma.update_online(experience)

# Periodic re-training
if steps % 1000 == 0:
    wma.finetune(replay_buffer)
```

## ðŸ“š Dodatkowe Zasoby

- [World Models Paper](https://worldmodels.github.io/)
- [Model Predictive Control](https://en.wikipedia.org/wiki/Model_predictive_control)
- [Reinforcement Learning](https://spinningup.openai.com/)
- [PyTorch Tutorial](https://pytorch.org/tutorials/)

---

**Pytania?** OtwÃ³rz Issue na GitHubie lub skonsultuj siÄ™ z prowadzÄ…cym!
